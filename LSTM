import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
from collections import Counter
import os
from argparse import Namespace
import pandas as pd



flags = Namespace(
    train_file='trainset.txt',
    test_file = 'testset.csv',
    seq_size = 1000,
    batch_size=5,
    embedding_size=4, #choose smaller
    lstm_size=64, # choose higher value; c
    gradients_norm=5,
    predict_top_k=2, # change in order to choose the max;  
)
    train_file='trainset.txt'
    test_file = 'testset.csv'
    seq_size=1000
    batch_size= 5
    embedding_size=64
    lstm_size=64
    gradients_norm=5
    predict_top_k=2
    



def get_data_from_file(train_file, batch_size, seq_size):
    with open(train_file, 'r') as f:
        text = f.read()
    
    def split(text): 
        return [char for char in text] 
    
    text = split(text)
    
    word_counts = Counter(text)
    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)
    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}
    vocab_to_int = {w: k for k, w in int_to_vocab.items()}
    n_vocab = len(int_to_vocab)

    print('Class size', n_vocab)
    
    int_text = [vocab_to_int[w] for w in text] 
    
    num_batches = int(len(int_text) / (seq_size * batch_size)) 
    in_text = int_text[:num_batches * batch_size * seq_size] 
    
    out_text = np.zeros_like(in_text)
    out_text[:-1] = in_text[1:]
    out_text[-1] = in_text[0]
    in_text = np.reshape(in_text, (batch_size, -1))
    out_text = np.reshape(out_text, (batch_size, -1))
    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text



def get_batches(in_text, out_text, batch_size, seq_size):
    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)
    for i in range(0, num_batches * seq_size, seq_size): # range(start, stop , step) 
        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]
        
        
class RNNModule(nn.Module):
    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):
        super(RNNModule, self).__init__()
        self.seq_size = seq_size
        self.lstm_size = lstm_size
        self.embedding = nn.Embedding(n_vocab, embedding_size)
        self.lstm = nn.LSTM(embedding_size,
                            lstm_size,
                            batch_first=True)
        self.dense = nn.Linear(lstm_size, n_vocab)
        
    
    def forward(self, x, prev_state):
        embed = self.embedding(x.long())
        
        output, state = self.lstm(embed, prev_state)
        logits = self.dense(output)
        return logits, state
    
    def zero_state(self, batch_size):
        return (torch.zeros(1, batch_size, self.lstm_size),
                torch.zeros(1, batch_size, self.lstm_size))
    
def get_loss_and_train_op(net, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)

    return criterion, optimizer
    



def main():
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(
        flags.train_file, flags.batch_size, flags.seq_size)
    
    net = RNNModule(n_vocab, flags.seq_size,
                    flags.embedding_size, flags.lstm_size)
    net = net.to(device)

    criterion, optimizer = get_loss_and_train_op(net, 0.01)

    iteration = 0
    
    epochs = range(3)
    for e in epochs: 
       
        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)
       
        state_h, state_c = net.zero_state(flags.batch_size)

        state_h = state_h.to(device)
        state_c = state_c.to(device)

        for x, y in batches: 
            
            iteration += 1
            
            net.train()

            optimizer.zero_grad()

            x = torch.tensor(x).to(device)
            y = torch.tensor(y).to(device)
           
            logits, (state_h, state_c) = net(x, (state_h, state_c))
           
            loss = criterion(logits.transpose(1, 2), y.long())

            state_h = state_h.detach()
            state_c = state_c.detach()

            loss_value = loss.item()

            loss.backward(retain_graph=True)

            optimizer.step()
        
            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)
            
            optimizer.step()
            
            if iteration % 100 == 0:
                print('Epoch: {}/{}'.format(e, epochs),
                      'Iteration: {}'.format(iteration),
                      'Loss: {}'.format(loss_value))
    
    # evaluate the model
    y_pred = predict(device, net, test_file, n_vocab, vocab_to_int, int_to_vocab, top_k=2)
    
    return y_pred



def predict(device, net, test_file, n_vocab, vocab_to_int, int_to_vocab, top_k):
    
    net.eval()

    state_h, state_c = net.zero_state(1)
    state_h = state_h.to(device)
    state_c = state_c.to(device)
    
    test_text = pd.read_csv (r'testset.csv')
    
    y_pred = np.zeros(1000, dtype = str)
    
    for i in range(1000): # because we have to make predictions for 1000 sequences
        
        test1 = test_text.iloc[i]
        word_input = test1.iloc[0]
        target = test1.iloc[1]
        
        def split(word_input): 
            return [char for char in word_input] 
    
        words = split(word_input)
    
        for w in words: 
            
            ix = torch.tensor([[vocab_to_int[w]]]).to(device)
            
            output, (state_h, state_c) = net(ix, (state_h, state_c))
            
        vals, top_ix = torch.topk(output[0], k=top_k)
        choices_vals = vals.tolist() 
        choices = top_ix.tolist()
        choice = np.random.choice(choices[0]) 
 
    
        y_pred[i] = int_to_vocab[choice]
        
    return y_pred
    
       

if __name__ == '__main__':
    y_pred = main()
